

# ğŸ§± PROJECT: `voice_agent_local`

## ğŸ¯ What weâ€™re building (recap)

A **local conversational AI** using:

* LM Studio (LLM)
* STT (speech â†’ text)
* TTS (text â†’ speech)

Clean structure:

* virtual environment
* requirements.txt
* modular code

---

# ğŸ”¹ STEP 0 â€” CREATE PROJECT FOLDER

Choose where you want it. Example:

```powershell
cd Desktop
mkdir voice_agent_local
cd voice_agent_local
```

Everything happens **inside this folder**.

---

# ğŸ”¹ STEP 1 â€” CREATE VIRTUAL ENVIRONMENT

```powershell
python -m venv venv
```

Activate it:

```powershell
venv\Scripts\activate
```

You should see:

```text
(venv) PS C:\...\voice_agent_local>
```

âœ”ï¸ Never skip this.

---

# ğŸ”¹ STEP 2 â€” PROJECT STRUCTURE (IMPORTANT)

Create this structure **exactly**:

```text
voice_agent_local/
â”‚
â”œâ”€ venv/
â”œâ”€ requirements.txt
â”œâ”€ README.md
â”‚
â””â”€ app/
   â”œâ”€ main.py
   â”œâ”€ conversation.py
   â”‚
   â”œâ”€ llm/
   â”‚  â””â”€ lmstudio.py
   â”‚
   â”œâ”€ stt/
   â”‚  â””â”€ whisper_stt.py
   â”‚
   â”œâ”€ tts/
   â”‚  â””â”€ piper_tts.py
   â”‚
   â””â”€ utils/
      â””â”€ audio.py
```

Create folders first, then empty `.py` files.

---

# ğŸ”¹ STEP 3 â€” `requirements.txt`

Create `requirements.txt` (top-level):

```txt
requests
sounddevice
numpy
faster-whisper
```

Install everything:

```powershell
pip install -r requirements.txt
```

If this fails â†’ stop and fix before continuing.

---

# ğŸ”¹ STEP 4 â€” LLM ENGINE (LM STUDIO)

### `app/llm/lmstudio.py`

```python
import requests

LMSTUDIO_URL = "http://localhost:1234/v1/chat/completions"
MODEL_NAME = "local-model"  # required by schema

def run_llm(messages):
    payload = {
        "model": MODEL_NAME,
        "messages": messages,
        "temperature": 0.2
    }

    response = requests.post(LMSTUDIO_URL, json=payload, timeout=60)
    response.raise_for_status()

    return response.json()["choices"][0]["message"]["content"]
```

ğŸ‘‰ Make sure **LM Studio API server is running**.

---

# ğŸ”¹ STEP 5 â€” SPEECH TO TEXT (STT)

### `app/stt/whisper_stt.py`

```python
import sounddevice as sd
import numpy as np
from faster_whisper import WhisperModel

model = WhisperModel(
    "small",
    device="cuda",        # change to "cpu" if needed
    compute_type="float16"
)

SAMPLE_RATE = 16000

def record_audio(seconds=5):
    audio = sd.rec(
        int(seconds * SAMPLE_RATE),
        samplerate=SAMPLE_RATE,
        channels=1,
        dtype=np.float32
    )
    sd.wait()
    return audio.flatten()

def speech_to_text():
    audio = record_audio()
    segments, _ = model.transcribe(audio)
    return " ".join(seg.text for seg in segments).strip()
```

We keep it **blocking + simple** for now.

---

# ğŸ”¹ STEP 6 â€” TEXT TO SPEECH (TTS)

### Folder setup

```text
voice_agent_local/
â””â”€ piper/
   â”œâ”€ piper.exe
   â””â”€ voices/
      â””â”€ en_US-amy-low.onnx
```

(You already know how to get Piper.)

---

### `app/tts/piper_tts.py`

```python
import subprocess
from pathlib import Path

BASE_DIR = Path(__file__).resolve().parents[2]
PIPER_EXE = BASE_DIR / "piper" / "piper.exe"
VOICE = BASE_DIR / "piper" / "voices" / "en_US-amy-low.onnx"
OUTPUT = BASE_DIR / "output.wav"

def speak(text: str):
    subprocess.run(
        [
            str(PIPER_EXE),
            "-m", str(VOICE),
            "-f", str(OUTPUT)
        ],
        input=text.encode("utf-8"),
        check=True
    )
```

Playback comes next (weâ€™ll add it).

---

# ğŸ”¹ STEP 7 â€” CONVERSATION LOOP (TEXT â†’ VOICE)

### `app/conversation.py`

```python
from llm.lmstudio import run_llm
from stt.whisper_stt import speech_to_text
from tts.piper_tts import speak

def voice_chat():
    messages = [
        {
            "role": "system",
            "content": "You are a friendly, concise conversational assistant."
        }
    ]

    while True:
        print("ğŸ§ Listening...")
        user_text = speech_to_text()

        if not user_text:
            continue

        print("You:", user_text)
        messages.append({"role": "user", "content": user_text})

        reply = run_llm(messages)
        print("AI:", reply)

        speak(reply)
        messages.append({"role": "assistant", "content": reply})
```

---

# ğŸ”¹ STEP 8 â€” ENTRY POINT

### `app/main.py`

```python
from conversation import voice_chat

if __name__ == "__main__":
    voice_chat()
```

Run it:

```powershell
python app/main.py
```

---

# âœ… IF THIS WORKS, YOU NOW HAVE

* clean project layout
* isolated venv
* reproducible installs
* modular agent
* voice in / voice out
* LLM swappable anytime

This is **real engineering**, not demos.

---

# ğŸ”œ NEXT PHASE OPTIONS

Say one and we continue **inside this structure**:

1ï¸âƒ£ Add **interrupt / barge-in**
2ï¸âƒ£ Stream audio instead of 5-second chunks
3ï¸âƒ£ Plug this into your **agent loop + tools**
4ï¸âƒ£ Add **memory (files / summaries)**
5ï¸âƒ£ Move to **LiveKit architecture**

Youâ€™re doing this the *right* way.


Good call. This step is **critical** â€” if STT/TTS arenâ€™t verified in isolation, everything above them becomes painful to debug.

Weâ€™ll do this **cleanly, step-by-step**:

1. Download
2. Verify files
3. Run **standalone tests** (no agent, no LLM)

No shortcuts.

---

# ğŸ”Š PART A â€” STT (Speech â†’ Text)

### Using **faster-whisper** (local, free, efficient)

---

## âœ… A1. Install dependencies (inside venv)

Make sure your venv is active:

```powershell
venv\Scripts\activate
```

Install STT deps:

```powershell
pip install faster-whisper sounddevice numpy
```

Verify install:

```powershell
pip show faster-whisper
```

If it shows version info â†’ OK.

---

## âœ… A2. Download Whisper model (automatic)

You **do not manually download** Whisper models.

The first time you run:

```python
WhisperModel("small")
```

It auto-downloads to:

```text
C:\Users\<you>\.cache\huggingface\hub\
```

This is correct behavior.

---

## âœ… A3. STT Standalone Test (IMPORTANT)

Create **temporary test file**:

### `app/stt/test_stt.py`

```python
from whisper_stt import speech_to_text

print("Say something...")
text = speech_to_text()
print("Recognized:", text)
```

Run:

```powershell
python app/stt/test_stt.py
```

### âœ… SUCCESS looks like:

* mic records ~5 seconds
* text prints correctly

### âŒ If it fails:

* No mic â†’ check Windows mic permissions
* CUDA error â†’ change `device="cpu"` in `whisper_stt.py`

---

# ğŸ”‰ PART B â€” TTS (Text â†’ Speech)

### Using **Piper** (offline, fast, free)

---

## âœ… B1. Download Piper (Windows)

Official releases:
ğŸ‘‰ [https://github.com/rhasspy/piper/releases](https://github.com/rhasspy/piper/releases)

Download:

```text
piper_windows_amd64.zip
```

Extract to:

```text
voice_agent_local/piper/
```

You must have:

```text
piper/
â”œâ”€ piper.exe
```

---

## âœ… B2. Download Voice Model

Official voices:
ğŸ‘‰ [https://huggingface.co/rhasspy/piper-voices](https://huggingface.co/rhasspy/piper-voices)

Example (recommended):

```text
en_US-amy-low.onnx
```

Place here:

```text
piper/voices/en_US-amy-low.onnx
```

Your structure must be:

```text
voice_agent_local/
â”œâ”€ piper/
â”‚  â”œâ”€ piper.exe
â”‚  â””â”€ voices/
â”‚     â””â”€ en_US-amy-low.onnx
```

---

## âœ… B3. Verify Piper Manually (NO PYTHON YET)

From project root:

```powershell
echo Hello from Piper | piper\piper.exe -m piper\voices\en_US-amy-low.onnx -f test.wav
```

Then play `test.wav`.

âœ”ï¸ If you hear voice â†’ Piper is correct
âŒ If not â†’ stop here and fix paths

---

## âœ… B4. Python TTS Standalone Test

Create:

### `app/tts/test_tts.py`

```python
from piper_tts import speak

speak("Piper text to speech is working.")
print("Audio generated.")
```

Run:

```powershell
python app/tts/test_tts.py
```

Then play `output.wav`.

---

# ğŸ” FINAL VERIFICATION CHECKLIST

Before moving on, confirm:

| Component             | Status |
| --------------------- | ------ |
| Microphone works      | âœ…      |
| STT prints text       | âœ…      |
| Piper CLI speaks      | âœ…      |
| Piper Python works    | âœ…      |
| No agent involved yet | âœ…      |

If **any** box is âŒ â†’ we stop and fix that part only.

---

# ğŸ§  WHY WE DID THIS

You just learned a **core systems lesson**:

> Always validate I/O subsystems independently before orchestration.

Most people skip this and suffer later.

---

## ğŸ”œ NEXT (YOU CHOOSE)


Great question â€” and this is one of those places where **â€œbetterâ€ depends on *what youâ€™re optimizing for***.

Iâ€™ll give you a **clear, opinionated answer**, grounded in:

* your hardware (GTX 1660, ~6 GB VRAM, 8 GB RAM)
* offline / LM Studio
* conversational agent (voice)
* learning-by-doing (not just chatting)

No marketing fluff.

---

# ğŸ§  FIRST: WHAT â€œBETTERâ€ MEANS FOR *YOU*

For your project, the LLM must be:

1. **Fast** (voice = latency matters)
2. **Stable** (no rambling, no infinite loops)
3. **Instruction-following** (agent-ready)
4. **Fits in VRAM** (or hybrid GPU/CPU without pain)
5. **Free & local**

So we care less about â€œbenchmarksâ€ and more about:

> *Does it behave well inside an agent loop and conversation?*

---

# ğŸ† RECOMMENDED MODELS (LM Studio)

## ğŸ¥‡ **Qwen 2.5 3B Instruct (GGUF / Q4â€“Q5)**

**Best overall choice for you**

### Why this one wins:

* Excellent instruction following
* Very stable in agent loops
* Low hallucination
* Strong conversational tone
* Designed for tool / agent use
* Works great at **3B** (huge deal)

### What to load in LM Studio:

* `Qwen2.5-3B-Instruct`
* Quantization: **Q4_K_M** or **Q5_K_M**

âœ… Fits easily on GTX 1660
âœ… Fast enough for voice
âœ… Scales later to tools & planning

ğŸ‘‰ **This should be your default model.**

---

## ğŸ¥ˆ **Llama 3.2 3B Instruct**

**Very good, slightly less disciplined**

### Pros:

* Natural conversation
* Good reasoning for size
* Stable responses

### Cons:

* Slightly weaker at strict instruction schemas
* More â€œchattyâ€ unless you prompt carefully

Good for:

* friendly voice assistants
* casual conversation

Less ideal for:

* tight agent control loops (but still usable)

---

## ğŸ¥‰ **Phi-3 Mini (3.8B)**

**Best reasoning-per-token, butâ€¦**

### Pros:

* Very strong logic
* Clean responses
* Efficient

### Cons:

* Dry personality
* Less conversational warmth
* Can feel robotic in voice mode

Good for:

* task agents
* logic-heavy tools

Less good for:

* friendly voice AI

---

# ğŸš« MODELS I DO *NOT* RECOMMEND (FOR NOW)

âŒ **7B models**
â†’ Too slow, VRAM pressure, voice latency suffers

âŒ **Mistral 7B (any flavor)**
â†’ Great text model, poor agent discipline locally

âŒ **Gemma 2B**
â†’ Weak instruction following

âŒ **Unquantized / FP16**
â†’ Waste of your hardware

---

# âš™ï¸ LM STUDIO SETTINGS (IMPORTANT)

For **Qwen 2.5 3B**, use:

* **Context length**: 4096
* **Temperature**: `0.2 â€“ 0.4`
* **Top-p**: `0.9`
* **GPU layers**: as high as possible (auto is fine)
* **Stop sequences** (optional):

  ```
  </s>
  ```

This makes it:

* deterministic
* calm
* agent-safe

---

# ğŸ§  WHY THIS MODEL WORKS WELL WITH YOUR AGENT LOOP

Qwen models are trained with:

* explicit instruction schemas
* tool / agent-style reasoning
* low verbosity bias

That means:

* it stops when told
* it doesnâ€™t fight your loop
* it behaves predictably

This matters **more than raw intelligence**.

---

# âœ… FINAL RECOMMENDATION (TL;DR)

If you want **one model to commit to**:

> **Qwen2.5-3B-Instruct (Q4_K_M)**
> via **LM Studio**

Use it for:

* conversation
* agent loops
* tools
* memory
* later LiveKit integration

---

## ğŸ”œ NEXT STEP (LOGICAL ORDER)

Next, we should:

1. **Lock the model**
2. Tune **system prompt for voice**
3. Add **interrupt / barge-in**
4. Then plug it back into the **agent loop**

Great question â€” and this is one of those places where **â€œbetterâ€ depends on *what youâ€™re optimizing for***.

Iâ€™ll give you a **clear, opinionated answer**, grounded in:

* your hardware (GTX 1660, ~6 GB VRAM, 8 GB RAM)
* offline / LM Studio
* conversational agent (voice)
* learning-by-doing (not just chatting)

No marketing fluff.

---

# ğŸ§  FIRST: WHAT â€œBETTERâ€ MEANS FOR *YOU*

For your project, the LLM must be:

1. **Fast** (voice = latency matters)
2. **Stable** (no rambling, no infinite loops)
3. **Instruction-following** (agent-ready)
4. **Fits in VRAM** (or hybrid GPU/CPU without pain)
5. **Free & local**

So we care less about â€œbenchmarksâ€ and more about:

> *Does it behave well inside an agent loop and conversation?*

---

# ğŸ† RECOMMENDED MODELS (LM Studio)

## ğŸ¥‡ **Qwen 2.5 3B Instruct (GGUF / Q4â€“Q5)**

**Best overall choice for you**

### Why this one wins:

* Excellent instruction following
* Very stable in agent loops
* Low hallucination
* Strong conversational tone
* Designed for tool / agent use
* Works great at **3B** (huge deal)

### What to load in LM Studio:

* `Qwen2.5-3B-Instruct`
* Quantization: **Q4_K_M** or **Q5_K_M**

âœ… Fits easily on GTX 1660
âœ… Fast enough for voice
âœ… Scales later to tools & planning

ğŸ‘‰ **This should be your default model.**

---

## ğŸ¥ˆ **Llama 3.2 3B Instruct**

**Very good, slightly less disciplined**

### Pros:

* Natural conversation
* Good reasoning for size
* Stable responses

### Cons:

* Slightly weaker at strict instruction schemas
* More â€œchattyâ€ unless you prompt carefully

Good for:

* friendly voice assistants
* casual conversation

Less ideal for:

* tight agent control loops (but still usable)

---

## ğŸ¥‰ **Phi-3 Mini (3.8B)**

**Best reasoning-per-token, butâ€¦**

### Pros:

* Very strong logic
* Clean responses
* Efficient

### Cons:

* Dry personality
* Less conversational warmth
* Can feel robotic in voice mode

Good for:

* task agents
* logic-heavy tools

Less good for:

* friendly voice AI

---

# ğŸš« MODELS I DO *NOT* RECOMMEND (FOR NOW)

âŒ **7B models**
â†’ Too slow, VRAM pressure, voice latency suffers

âŒ **Mistral 7B (any flavor)**
â†’ Great text model, poor agent discipline locally

âŒ **Gemma 2B**
â†’ Weak instruction following

âŒ **Unquantized / FP16**
â†’ Waste of your hardware

---

# âš™ï¸ LM STUDIO SETTINGS (IMPORTANT)

For **Qwen 2.5 3B**, use:

* **Context length**: 4096
* **Temperature**: `0.2 â€“ 0.4`
* **Top-p**: `0.9`
* **GPU layers**: as high as possible (auto is fine)
* **Stop sequences** (optional):

  ```
  </s>
  ```

This makes it:

* deterministic
* calm
* agent-safe

---

# ğŸ§  WHY THIS MODEL WORKS WELL WITH YOUR AGENT LOOP

Qwen models are trained with:

* explicit instruction schemas
* tool / agent-style reasoning
* low verbosity bias

That means:

* it stops when told
* it doesnâ€™t fight your loop
* it behaves predictably

This matters **more than raw intelligence**.

---

# âœ… FINAL RECOMMENDATION (TL;DR)

If you want **one model to commit to**:

> **Qwen2.5-3B-Instruct (Q4_K_M)**
> via **LM Studio**

Use it for:

* conversation
* agent loops
* tools
* memory
* later LiveKit integration

---

## ğŸ”œ NEXT STEP (LOGICAL ORDER)

Next, we should:

1. **Lock the model**
2. Tune **system prompt for voice**
3. Add **interrupt / barge-in**
4. Then plug it back into the **agent loop**

Say:

* **â€œLock Qwen and tune promptâ€**
  or
* **â€œCompare two models liveâ€**

Youâ€™re building this the *right* way â€” slow, solid, reusable.

